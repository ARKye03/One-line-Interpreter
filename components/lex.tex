\begin{center}
    \centering
    \section{Lexer}
\end{center}

\subsection{Lexer Summary}

A Lexer, also known as a lexical analyzer, tokenizer, or scanner, is a program or function that breaks down input code into a sequence of tokens. These tokens are meaningful units of code. 

In the process of parsing programming languages, this task is traditionally split up into two phases: the lexing stage and the parsing stage. The lexer's job is simpler than the parser's. It turns the meaningless string into a flat list of things like ``number literal'', ``string literal'', ``identifier'', or ``operator''. It can also recognize reserved identifiers (``keywords'') and discard whitespace. 

The output of the lexer is used as the input for the parser. The parser then has the much harder job of turning the stream of ``tokens'' produced by the lexer into a parse tree representing the structure of the parsed language. This separation allows the lexer to do its job well and for the parser to work on a simpler, more meaningful input than the raw text.

More info about:
\begin{itemize}
    \item \href{https://dev.to/cad97/what-is-a-lexer-anyway-4kdo}{What's a Lexer}
    \item \href{https://accu.org/journals/overload/26/145/balaam_2510/}{The Lexer - ACCU}
    \item \href{https://www.programmingassignmenthelper.com/blog/lexer-and-parser-in-c/}{Extra blog}
\end{itemize}

\lstset{style=sharpc}
\subsection{Now, my Lexer}
    First I've defined some simple Token types in this ``enum'':
    \begin{lstlisting}
        public enum TokenType
        {
            FLinq, //Link token for function declaration
            ComparisonOperator,
            Number, // Number
            StringLiteral, // Literally a string
            FunctionDeclaration, // Well, deprecated
            LetKeyword, 
            IfKeyword, 
            ElseKeyword, 
            PrintKeyword, 
            InKeyword,
            FunctionKeyword,
            Operator, // '+','-','*','/','^', '='...
            Punctuation, // '(',')' & ','
            Identifier, //Used for variables name and functions value
            EOL, //';'
            EOF, //Not yet
            Semicolon, //Idk man/woman
            Separator // '@', and in a future, "||", "&&"...
        }
    \end{lstlisting}
    Basically, my Tokens, have a few properties:
    \begin{lstlisting}
        public TokenType type; // From enum
        public string value; // Text of the token
        public int line; // Line position, currently 1, always
        public int column; // Column position
    \end{lstlisting}
    \texttt{Note:} ToString override is there to test tokens only.

    \newpage
    \begin{center}
        \centering
        Some functions of my lexer
    \end{center}
\begin{itemize}
    \item advance{()}: One char to the right.
    \item skip\_whitespace{()}: Jumps to next non-blank char.
    \item peek{()}: Takes a look to next token, without taking it.
    \item unget\_token{()}: Drops next token, to use it later.
    \item Some Token functions. Take a summary along the code itself
\end{itemize}

\begin{center}
    \centering
    The main function
\end{center}

The \hbox{get\_next\_token{()}} not that simple C\# method that tokenizes a source code string for a programming language interpreter. This method reads characters from the source code and categorizes them into different types of tokens like operators, punctuation, keywords, etc.

If you're asking for an explanation of this code, here's a brief overview:
\begin{itemize}
\item  The `get\_next\_token' method is used to get the next token from the source code.
\item  It first checks if there are any tokens already read and not processed. If so, it returns the first one and removes it from the list.
\item  If there are no pre-read tokens, it starts reading the source code character by character.
\item  Depending on the current character, it categorizes it into a token type (like operator, punctuation, keyword, etc.) and returns a new token of that type.
\item  It also handles multi-character tokens like $`=>`$, $`==`$, $`<=`$, $`>=`$, and $`!=`$.
\item  If it encounters a newline character (`\\n`), it increments the line count and resets the column count.
\item  If it encounters an unrecognized character, it prints an error message.
\item  If it reaches the end of the source code, it returns an EOF (end-of-file) token.
\end{itemize}

The `function\_declaration{()}': used to parse a function declaration in the source code of a programming language. Here's a brief overview of what it does:
\begin{itemize}
    \item  It first checks if the next token is a function keyword and then expects an identifier (the function name).
    \item  It reads the function name by appending all the non-whitespace characters.
    \item  It then checks for an opening parenthesis `(' to start the function parameters list.
    \item  It reads all the parameters of the function, separated by commas `,`, and adds them to a list.
    \item  It then expects a $`=>`$ token, which presumably indicates the start of the function body.
    \item  It reads all the tokens of the function body until it encounters a semicolon `;' or an EOF (end-of-file) token, indicating the end of the function body.
    \item  Finally, it returns a `FunctionToken' that represents the function declaration, including its name, parameters, and body. Note: This `Token', turns to `DFunction'.
\end{itemize}
